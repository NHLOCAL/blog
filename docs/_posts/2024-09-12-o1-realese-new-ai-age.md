---
layout: post
title: "הי, עצור! עידן AI חדש לפניך!"
date: 2024-09-12
categories: AI
tags: AI OpenAI ChatGPT LLM
featured_image: "/assets/images/o1-openai.webp"
---
## **חברת OPENAI שחררו מודל חדש ניסיוני בשם O1!**

נכון לעכשיו הוא זמין למנויים בתשלום דרך צ'אט GPT, בעתיד הוא צפוי להיות משוחרר לכלל המשתמשים.

שימו לב למדדים שלו בהשוואה לדגם האחרון שלהם - GPT4O
מדובר בקפיצה בלתי נתפסת ביכולות כמו קידוד, מתמטיקה, מדעים, פיזיקה ועוד!

<img src="/assets/images/o1-evals.png" alt="תיאור התמונה" style="max-width:70%;">
### הסבר מפורט על הגרפים
**(הסיכום נכתב באמצעות צ'אט GPT, השתדלתי לתקן את הסיכום, אך יתכן וישנם עדיין טעויות קלות במספרים):**

#### 1. מבחני תחרויות חשיבה:
**תחרות מתמטיקה (AIME 2024):**
דגם GPT-4o הצליח רק ב-13.4% מהמבחן.
דגם o1 לעומתו הצליח ב-83.3%, שזה שיפור עצום. זו דוגמה לתחרות מתמטיקה מתקדמת מאוד, ודגם o1 מצליח בצורה מרשימה לעומת היכולות הקודמות של GPT-4o.
גרסת "preview" של o1 הצליחה במבחן עם ציון של 56.7%, מה שמעיד שגם בשלבי הפיתוח הוא היה טוב יותר מ-GPT-4o.

**תחרות קוד (CodeForces):**
גם כאן, הביצועים של GPT-4o היו נמוכים – הוא השיג רק 11%.
דגם o1 קיבל ציון גבוה מאוד של 89%, שזה שיפור ענק בתחרות שמצריכה פתרון בעיות קוד מורכבות בזמן אמת.

**שאלות ברמת דוקטורט (PhD) במדעים (GPQA Diamond):**
GPT-4o השיג 56.1% בשאלות מדעיות מסובכות ברמה אקדמית מתקדמת.
דגם o1 הצליח להגיע ל-78%, מה שמראה שהוא טוב יותר אפילו מאדם מומחה (שהצליח רק ב-69.7%).

### 2. מבחנים במערכות למידת מכונה (ML Benchmarks):
יש כאן סדרה של מבחנים שמודדים את יכולות הבינה המלאכותית במתמטיקה, בפתרון בעיות מבוססות טקסט, ובהבנה של מערכות מורכבות.

ב-MATH-500, למשל, GPT-4o השיג 60.3%, אבל דגם o1 קיבל ציון מדהים של 94.8%.
ב-MMLU, מבחן רחב נוסף, GPT-4o קיבל 88%, בעוד ש-dgmo o1 שיפר והגיע ל-92.3%.

### 3. שאלות מדעיות ברמת PhD (GPQA Diamond):
מדובר בבחינות בתחומים כמו כימיה, פיזיקה, וביולוגיה, שנשאלות ברמה אקדמית גבוהה (ברמת דוקטורט).

בכימיה, לדוגמה, GPT-4o קיבל 40.2%, בעוד ש-dgmo o1 השיג שיפור משמעותי והגיע ל-64.7%.

בפיזיקה הביצועים של o1 היו גם טובים יותר, כאשר הוא קיבל 92.8% בהשוואה ל-GPT-4o עם 59.5%.

בביולוגיה, דגם o1 קיבל ציון של 69.2%, הרבה יותר טוב מ-GPT-4o שקיבל 61.6%.

### 4. מבחני השכלה כללית (Exams):
כאן נמדדו היכולות של שני הדגמים במבחנים אקדמיים כמו מבחן AP, SAT ו-LSAT.

לדוגמה, במבחן AP English Literature, דגם GPT-4o קיבל 68.7%, בעוד ש-dgmo o1 שיפר מעט וקיבל 69%.

במבחנים אחרים, כמו AP Physics 2, דגם o1 הגיע ל-89%, הרבה מעל 69% של GPT-4o.

במבחן SAT במתמטיקה, o1 היה מדויק לגמרי עם ציון של 100%, בעוד ש-GPT-4o קיבל ציון דומה של 100%, אבל מדובר בבחינה יחסית פשוטה.

### 5. קטגוריות של מבחן MMLU:
מדובר במבחנים שבוחנים ידע בתחומים שונים כמו לוגיקה פורמלית, יחסי ציבור, כלכלה, מוסר, ועוד. כאן רואים ש-dgmo o1 משפר את התוצאות ברוב הקטגוריות.

לדוגמה, בתחום של מוסר, o1 קיבל 85.8%, בעוד ש-GPT-4o קיבל 80%.

בקטגוריות אחרות, כמו יחסי ציבור, o1 קיבל 75%, שוב הרבה יותר טוב מ-GPT-4o.

### סיכום:
דגם o1 מציג שיפורים משמעותיים כמעט בכל תחום שנבדק בהשוואה ל-GPT-4o, במיוחד בתחומים שדורשים חשיבה עמוקה, פתרון בעיות מתמטיות מורכבות, ותשובות לשאלות מדעיות ברמה אקדמית גבוהה. הביצועים של o1 קרובים מאוד, ולעיתים עוקפים, את הביצועים של מומחים אנושיים בתחומים מסוימים.

בנוסף, השיפור שנראה בתמונה מתמקד בכך ש-o1 מצליח להיות יותר מדויק, להבין תהליכים מורכבים בצורה טובה יותר, ולספק תשובות ברורות ומהירות יותר, מה שיכול להפוך אותו לכלי יותר שימושי בתחומים כמו תחרויות מתמטיקה, פיזיקה, קידוד, והבנה מדעית רחבה.

----

#### מידע נוסף:

- [Introducing O1: OpenAI’s new reasoning model series for developers and enterprises on Azure](https://azure.microsoft.com/en-us/blog/introducing-o1-openais-new-reasoning-model-series-for-developers-and-enterprises-on-azure/){:target="_blank" rel="noopener"}
- [OpenAI unveils a model that can fact-check itself (TechCrunch)](https://techcrunch.com/2024/09/12/openai-unveils-a-model-that-can-fact-check-itself/){:target="_blank" rel="noopener"}
- [OpenAI Platform Docs: Reasoning](https://platform.openai.com/docs/guides/reasoning){:target="_blank" rel="noopener"}